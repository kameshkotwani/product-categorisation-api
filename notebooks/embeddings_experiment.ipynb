{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fa023d7",
   "metadata": {},
   "source": [
    "### Notebook to train using embeddings using FastText and CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5175cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# there are total of 1451 rows in dataset, I have removed 49 rows for which image was not downloadable\n",
    "def get_data_without_images()->list:\n",
    "    df = pd.read_csv(\"../data/processed/cleaned_results.csv\")\n",
    "    X = df[['name','brandName']]\n",
    "    Y = df['categoryName']\n",
    "\n",
    "    # Cannot use Statrify here since there are categories with only 1 value, so it's not possible to split them\n",
    "    return train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e41e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42046059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this working fine the data is split into 80% train and 20% test\n",
    "X_train,X_test,y_train,y_test = get_data_without_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21498de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d19442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_text(text):\n",
    "    return [token.text.lower() for token in nlp(str(text)) if not token.is_punct and not token.is_space]\n",
    "\n",
    "def tokenize_column(column):\n",
    "    return [tokenize_text(text) or ['<unk>'] for text in column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b0c5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenisation\n",
    "# this can be optimised by using np.array\n",
    "product_tokens = tokenize_column(X_train['name'])\n",
    "brand_tokens = tokenize_column(X_train['brandName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b53269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Step 3: Train FastText model\n",
    "# -------------------------------\n",
    "combined_corpus = product_tokens + brand_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "411c35b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a fast-text model to generate word embeddings\n",
    "ft_model:FastText = FastText(\n",
    "    sentences=combined_corpus,\n",
    "    vector_size=100,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    sg=4,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a389dbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19380973, -0.17312033,  0.14489503, -0.2472115 ,  0.55954975,\n",
       "       -0.7385266 , -0.16032806,  0.12783523,  1.0224581 , -0.03216781,\n",
       "        0.12178689,  0.5559957 , -0.12568694, -0.07842773, -0.37182537,\n",
       "        0.3525978 , -0.31145954, -2.010154  , -0.81038016, -0.92585725,\n",
       "        0.19795062,  0.43108428, -1.0330576 , -0.8915576 , -0.7708586 ,\n",
       "       -0.1356315 , -1.3975393 , -1.343079  , -0.14235164, -0.15676445,\n",
       "       -0.7619604 ,  0.4027679 ,  0.11804128,  0.32898068,  0.82878506,\n",
       "        0.1668406 , -1.1847748 ,  0.47173795, -0.5412962 ,  0.24479671,\n",
       "        0.04600191,  0.11579774,  0.0387099 , -0.6512558 , -0.14157335,\n",
       "       -0.8928325 , -0.0169696 , -0.02891628, -0.26630569, -0.04782018,\n",
       "       -0.7522785 ,  0.35130247, -0.6882591 ,  0.73537785, -0.6475959 ,\n",
       "       -0.7879068 ,  0.1858443 , -0.4022274 , -0.18064018,  0.23419975,\n",
       "       -0.81957287, -0.5313862 , -0.4055738 ,  0.9982064 , -0.12088407,\n",
       "        0.03194728,  0.5622262 , -0.19736207,  0.14331809, -0.8741291 ,\n",
       "       -0.01269009,  1.0708344 , -0.32231468, -0.74368536,  0.6231817 ,\n",
       "       -0.50590515, -0.16440949, -0.00456092, -0.4383927 ,  1.313157  ,\n",
       "        0.8096082 ,  0.15285529, -0.1889019 ,  0.634754  , -0.67357504,\n",
       "       -1.0358208 , -0.12388921,  1.0470805 ,  0.09347142,  0.1520542 ,\n",
       "       -0.3823695 ,  1.0051559 , -0.6662658 , -0.3574823 , -1.0698557 ,\n",
       "        1.4560603 ,  0.17750062, -0.3461249 ,  1.5302564 ,  0.31588694],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the word vector for a word\n",
    "ft_model.wv['revlon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b46f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Step 4: Get sentence vectors\n",
    "# -------------------------------\n",
    "def sentence_vector(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    vectors = [ft_model.wv[t] if t in ft_model.wv else ft_model.wv['<unk>'] for t in tokens]\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71492c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate sentence vectors for product and brand\n",
    "X_product = np.vstack([sentence_vector(text) for text in X_train['name']])\n",
    "X_brand = np.vstack([sentence_vector(text) for text in X_train['brandName']])\n",
    "X = np.concatenate([X_product, X_brand], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3609ed05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1160, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9855440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Step 5: Encode target labels\n",
    "# -------------------------------\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30e2b7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdd59a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pool for train data\n",
    "from catboost import Pool\n",
    "train_pool = Pool(\n",
    "    data=X,\n",
    "    label=y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "261d7065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.066901\n",
      "0:\tlearn: 0.1227962\ttotal: 153ms\tremaining: 2m 33s\n",
      "100:\tlearn: 0.9967362\ttotal: 12.1s\tremaining: 1m 47s\n",
      "200:\tlearn: 0.9991093\ttotal: 24s\tremaining: 1m 35s\n",
      "300:\tlearn: 0.9998120\ttotal: 35.3s\tremaining: 1m 22s\n",
      "400:\tlearn: 0.9999191\ttotal: 46.9s\tremaining: 1m 10s\n",
      "500:\tlearn: 1.0000000\ttotal: 58.2s\tremaining: 58s\n",
      "600:\tlearn: 1.0000000\ttotal: 1m 9s\tremaining: 46.2s\n",
      "700:\tlearn: 1.0000000\ttotal: 1m 20s\tremaining: 34.5s\n",
      "800:\tlearn: 1.0000000\ttotal: 1m 32s\tremaining: 22.9s\n",
      "900:\tlearn: 1.0000000\ttotal: 1m 43s\tremaining: 11.4s\n",
      "999:\tlearn: 1.0000000\ttotal: 1m 54s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f8ec31fcad0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step 6: Train CatBoostClassifier\n",
    "# -------------------------------\n",
    "model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"Accuracy\",\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    verbose=100,\n",
    "    task_type='GPU',\n",
    "    devices='0',\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "\n",
    "model.fit(train_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5e56c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Step 7: Prediction function\n",
    "# -------------------------------\n",
    "def predict_category(product_text, brand_text):\n",
    "    prod_vec = sentence_vector(product_text)\n",
    "    brand_vec = sentence_vector(brand_text)\n",
    "    full_vec = np.concatenate([prod_vec, brand_vec]).reshape(1, -1)\n",
    "    pred_class = model.predict(full_vec)\n",
    "    return label_encoder.inverse_transform(pred_class.astype(int))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca166d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: Hair Dye\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamesh/interview-tests/qogita/.venv/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "example_product = \"Revlon hair dye 10ml\"\n",
    "example_brand = \"Revlon\"\n",
    "prediction = predict_category(example_product, example_brand)\n",
    "print(f\"Predicted category: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0a2ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a test_pool using X_test\n",
    "def create_test_pool(X_test):\n",
    "    X_product = np.vstack([sentence_vector(text) for text in X_test['name']])\n",
    "    X_brand = np.vstack([sentence_vector(text) for text in X_test['brandName']])\n",
    "    X = np.concatenate([X_product, X_brand], axis=1)\n",
    "    test_pool = Pool(\n",
    "        data=X,\n",
    "        label=y_test,\n",
    "    )\n",
    "    return test_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b093f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pool = create_test_pool(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24a46e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afd4c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cat = label_encoder.inverse_transform(y_pred.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab07f3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93faa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                          Acne       0.00      0.00      0.00         1\n",
      "        Anti-Aging Facial Care       0.00      0.00      0.00         2\n",
      "               Anti-Aging Mask       0.00      0.00      0.00         1\n",
      "              Anti-Aging Serum       0.00      0.00      0.00         5\n",
      "                  Baby & Child       0.00      0.00      0.00         1\n",
      "                     Bleaching       0.00      0.00      0.00         2\n",
      "                   Body Butter       0.00      0.00      0.00         1\n",
      "                   Body Lotion       0.83      1.00      0.91         5\n",
      "                     Body Mist       0.00      0.00      0.00         0\n",
      "               Cleansing Cream       0.00      0.00      0.00         3\n",
      "                   Color Rinse       0.00      0.00      0.00         1\n",
      "                     Concealer       0.00      0.00      0.00         1\n",
      "             Concealer Brushes       0.00      0.00      0.00         1\n",
      "                   Conditioner       0.45      1.00      0.62         5\n",
      "                    Contouring       0.00      0.00      0.00         1\n",
      "                     Day Cream       0.00      0.00      0.00         1\n",
      "   Deodorant & Anti-Perspirant       1.00      0.60      0.75         5\n",
      "                   Dry Shampoo       0.00      0.00      0.00         2\n",
      "                Eau De Cologne       0.50      0.50      0.50         2\n",
      "                 Eau De Parfum       0.84      0.93      0.88        57\n",
      "               Eau De Toilette       0.85      0.61      0.71        18\n",
      "             Extrait De Parfum       0.00      0.00      0.00         0\n",
      "                     Eye Cream       0.00      0.00      0.00         1\n",
      "                    Eye Pencil       0.33      1.00      0.50         1\n",
      "            Eye Sets & Pallets       0.67      1.00      0.80         2\n",
      "                Eyebrow Pencil       0.00      0.00      0.00         2\n",
      "                Eyebrow Powder       0.00      0.00      0.00         0\n",
      "                      Eyeliner       1.00      0.67      0.80         3\n",
      "                     Eyeshadow       1.00      0.89      0.94         9\n",
      "                    Face Cream       0.33      0.78      0.47         9\n",
      "           Face Sun Protection       1.00      1.00      1.00         1\n",
      "              Facial Care Sets       0.00      0.00      0.00         3\n",
      "                    Facial Oil       0.00      0.00      0.00         1\n",
      "        Facial Scrub & Peeling       0.00      0.00      0.00         1\n",
      "                  Facial Spray       0.00      0.00      0.00         2\n",
      "                    Foundation       0.79      0.94      0.86        16\n",
      "                     Fragrance       0.00      0.00      0.00         1\n",
      "                Fragrance Sets       0.75      0.67      0.71         9\n",
      "                    Glow Serum       0.00      0.00      0.00         1\n",
      "                      Hair Dye       0.84      0.93      0.89        29\n",
      "                    Hair Masks       0.58      0.88      0.70         8\n",
      "         Hair Oil & Hair Serum       0.00      0.00      0.00         0\n",
      "                    Hair Tonic       0.00      0.00      0.00         1\n",
      "                     Hairspray       0.00      0.00      0.00         2\n",
      "                    Hand Cream       1.00      0.50      0.67         2\n",
      "                   Highlighter       0.00      0.00      0.00         1\n",
      "         Hyaluronic Acid Serum       0.00      0.00      0.00         0\n",
      "               Hydrating Serum       0.00      0.00      0.00         1\n",
      "                      Lip Balm       0.00      0.00      0.00         4\n",
      "                     Lip Gloss       0.50      0.60      0.55         5\n",
      "                     Lip Liner       1.00      0.60      0.75         5\n",
      "                       Lip Oil       0.00      0.00      0.00         2\n",
      "                   Lip Plumper       0.00      0.00      0.00         1\n",
      "                      Lipstick       0.72      1.00      0.84        18\n",
      "                Makeup Mirrors       0.00      0.00      0.00         1\n",
      "                       Mascara       1.00      1.00      1.00         1\n",
      "                Micellar Water       0.00      0.00      0.00         0\n",
      "                   Nail Polish       1.00      1.00      1.00        10\n",
      "               Neurodermatitis       0.00      0.00      0.00         1\n",
      "                   Night Cream       0.00      0.00      0.00         1\n",
      "                        Powder       0.00      0.00      0.00         1\n",
      "                    Scalp Care       0.00      0.00      0.00         0\n",
      "                       Shampoo       0.71      0.42      0.53        12\n",
      "                       Shaving       1.00      0.50      0.67         2\n",
      "                    Shower Gel       0.50      0.67      0.57         3\n",
      "                    Shower Oil       0.00      0.00      0.00         1\n",
      "                          Soap       0.00      0.00      0.00         0\n",
      "                Styling Creams       0.00      0.00      0.00         1\n",
      "                Styling Sprays       0.00      0.00      0.00         0\n",
      "Toothbrushes & Tongue Cleaners       1.00      1.00      1.00         1\n",
      "\n",
      "                      accuracy                           0.68       291\n",
      "                     macro avg       0.29      0.30      0.28       291\n",
      "                  weighted avg       0.65      0.68      0.65       291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d7e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40d102e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Generate the classification report as a dictionary\n",
    "report = classification_report(y_test, y_pred_cat, zero_division=0, output_dict=True)\n",
    "\n",
    "# Write the report to a JSON file\n",
    "with open(\"classification_report.json\", \"w\") as json_file:\n",
    "    json.dump(report, json_file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
